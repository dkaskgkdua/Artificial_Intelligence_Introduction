# Reinforcement Learning: Q-Learning

## 1. 머신러닝(기계학습) 소개

### 핵심 개념

* **인공지능 (Artificial Intelligence)**: 사람이 수행하는 지능적인 행동을 컴퓨터나 기계가 할 수 있도록 하는 학문입니다.
* **머신러닝 (Machine Learning)**: 인공지능을 구현하는 한 방법론으로, 컴퓨터가 대량의 데이터를 통해 스스로 학습하는 알고리즘과 기술을 개발하는 분야입니다.

### 머신러닝의 핵심 요소

머신러닝은 주로 두 가지 기본 개념에 중점을 둡니다.

* **표현 (Representation)**: 학습을 위해 주어진 데이터를 적절한 방식으로 표현하는 것을 의미합니다.
* **일반화 (Generalization)**: 학습에 사용되지 않은, 아직 주어지지 않은 새로운 데이터에 대해서도 효과적으로 처리하는 능력을 의미합니다.


## 2. 머신러닝의 분류

머신러닝은 학습 방법에 따라 크게 세 가지로 분류할 수 있습니다.

* **지도 학습 (Supervised Learning)**

    * **학습 방식**: 목표값, 즉 '레이블(label)'이 명시된 데이터를 사용하여 학습합니다.
    * **프로세스**: 레이블이 지정된 훈련 데이터셋으로 모델을 구축한 뒤, 별도의 테스트 데이터셋으로 모델의 예측 성능을 평가합니다.

* **비지도 학습 (Unsupervised Learning)**

    * **학습 방식**: 레이블이 제시되지 않은 데이터를 사용하여 학습합니다.
    * **프로세스**: 모델이 데이터 내에 숨겨진 패턴, 구조, 또는 그룹을 스스로 찾아냅니다.

* **강화 학습 (Reinforcement Learning)**

    * **학습 방식**: '에이전트(agent)'가 자신이 수행한 행동에 대해 '보상(reward)'을 받으며 점차적으로 효율적인 방식으로 행동을 강화시키는 학습 방법입니다.
    * **프로세스**: 에이전트가 '환경(environment)'과 상호작용하며, 더 높은 보상을 얻는 방향으로 행동을 수정해 나갑니다.


## 3. 강화학습이란?

### 정의

강화학습은 에이전트가 반복적인 시도를 통해 시행착오를 겪으며 목표에 도달하는 방법을 학습하는 기계 학습의 한 형태입니다. 외부 환경으로부터 주어지는 **보상**을 통해 자신의 행동을 개선해 나갑니다. 핵심 원리는 **경험을 통한 학습(Learn via experiences!)** 입니다.

### 주요 특징

* **지연된 보상 (Delayed reward)**: 시행착오 과정에서 선택한 행동이 적합한 선택이었는지 즉시 알 수 없습니다.
* **불확실성**: 현재 선택한 행동의 결과를 정확히 알 수 없어도 학습이 가능합니다.
* **평생 학습 (Life-long learning)**: 지속적으로 적응하고 개선해야 하는 학습 시나리오에 활용될 수 있습니다.

### 강화학습 프레임워크

강화학습 과정은 **에이전트**와 **환경** 사이의 지속적인 상호작용으로 이루어집니다.

1.  에이전트는 환경으로부터 현재 **상태(state)** 를 인식(percept)합니다.
2.  상태를 바탕으로 에이전트는 특정 **행동(action)** 을 수행합니다.
3.  환경은 새로운 상태로 바뀌고 에이전트에게 **보상(reward)** 을 제공합니다.
4.  에이전트는 이 보상과 새로운 상태를 바탕으로 다음 행동을 결정하는 방법을 학습합니다.


## 4. Q-러닝 (Q-Learning)

Q-러닝은 강화학습 분야에서 가장 널리 사용되는 핵심 알고리즘 중 하나입니다.

### 동작 원리

이 알고리즘은 에이전트가 어떤 상황에서 어떤 행동을 취해야 하는지를 알려주는 정책(policy)을 학습합니다. 이는 특정 상태 `s`에서 특정 행동 `a`를 했을 때의 "가치" 또는 기대되는 장기적 보상을 나타내는 **행동-가치 함수**인 $Q(s, a)$를 학습함으로써 이루어집니다.

* **학습 과정**: 에이전트는 시행착오를 통해 학습합니다. 특정 행동을 선택하고, 환경으로부터 보상을 받은 뒤, 이 보상을 사용하여 방금 경험한 상태-행동 쌍의 Q값을 업데이트합니다. 이 업데이트는 행동 순서에 따라 보상을 전파하는 형태로 이루어집니다.
* **Q-함수**: $Q(s, a)$는 행동 `a`를 취함으로써 얻는 즉각적인 보상과 그로 인해 변화된 새로운 상태 $s'$에서 얻을 수 있는 잠재적 최대 보상의 합으로 정의됩니다.
* **최적 정책**: Q-함수 학습이 완료되면(즉, 수렴하면), 어떤 상태 `s`에 있든지 $Q(s, a)$ 값을 최대화하는 행동 `a`를 선택하는 것이 최적의 정책이 됩니다.

### Q-Learning Algorithm

Q-러닝의 핵심은 경험을 통해 Q값을 개선해 나가는 업데이트 규칙입니다.

**Q값 업데이트 공식:**
$$Q(s, a) = r(s, a) + \gamma \max_{a'} Q(s', a')$$

여기서 각 항목의 의미는 다음과 같습니다:

* $Q(s, a)$: 상태 `s`에서 행동 `a`를 취했을 때의 Q값.
* $r(s, a)$: 상태 `s`에서 행동 `a`를 선택했을 때 주어지는 즉각적인 보상(Immediate reward).
* $\\gamma$ (감마): 할인율(discount factor)로, 0과 1 사이의 값을 가집니다. 미래 보상의 중요도를 결정하며, 1에 가까울수록 미래 보상에 더 큰 가중치를 둡니다.
* $s'$: 행동 `a`를 취한 후의 새로운 상태.
* $\\max\_{a'} Q(s', a')$: 새로운 상태 $s'$에서 가능한 다음 행동 $a'$ 중 최선을 선택했을 때 얻을 수 있는 최대 Q값. 이는 지연된 미래 보상을 의미합니다.

**알고리즘 단계:**

1.  모든 $Q(s, a)$ 값을 0으로 초기화하여 Q-테이블을 생성합니다.
2.  Q-테이블이 수렴할 때까지 여러 에피소드(학습 과정)를 반복합니다.
    a. 현재 상태 `s`에서 시작합니다.
    b. 가능한 행동 `a`를 하나 선택하여 실행합니다.
    c. 환경으로부터 즉각적인 보상 `r`을 받습니다.
    d. 새로운 상태 `s'`를 감지합니다.
    e. 공식을 사용하여 Q-테이블을 업데이트합니다: $Q(s, a) \\leftarrow r + \\gamma \\max\_{a'} Q(s', a')$.
    f. 현재 상태를 새로운 상태로 변경합니다: $s \\leftarrow s'$.

### Q-러닝 예제

에이전트가 간단한 그리드 월드에서 임의의 시작점으로부터 목표 지점(S6)까지 가는 최단 경로를 학습하는 예제를 살펴보겠습니다.

* **환경**: 6개의 방으로 이루어진 그리드 (S1 ~ S6).
* **목표**: S6 상태에 도달하기.
* **보상**: S6에 들어갈 경우 100의 보상을 받고, 다른 모든 이동의 보상은 0입니다.
* **할인율 ($\\gamma$)**: 이 예제에서는 0.5로 설정합니다.

**초기 상태**: Q-테이블의 모든 값은 0으로 초기화됩니다.

| s,a | Q(s,a) |
| :--- | :--- |
| S1,a12 | 0 |
| S1,a14 | 0 |
| S2,a21 | 0 |
| ... | 0 |
| S5,a56 | 0 |

**학습 과정 하이라이트:**

1.  **첫 번째 에피소드**: 에이전트가 S1 -> S2 -> S3 -> S6 경로로 무작위로 이동했다고 가정합니다.

    * S1->S2, S2->S3 이동은 보상이 0이고 초기 Q값들도 모두 0이므로 Q값은 변하지 않습니다.
    * 에이전트가 S3에서 S6로 이동할 때(행동 a36), 100의 보상을 받습니다. 이때 Q값이 업데이트됩니다.
        * $Q(S3, a36) = r = 100$ (최종 상태에서는 미래 보상이 없으므로 즉각적인 보상값이 Q값이 됩니다).

2.  **두 번째 에피소드**: 이제 에이전트가 S2에서 시작하여 S3로 이동했다고 가정합니다.

    * 에이전트는 상태 S2에서 행동 a23을 취해 상태 S3에 도달합니다.
    * $Q(S2, a23)$ 값을 업데이트합니다. 에이전트는 S3에서 얻을 수 있는 최대 Q값을 참조하며, 현재는 $Q(S3, a36) = 100$입니다.
    * 업데이트 계산: $Q(S2, a23) = r + \\gamma \\max(Q(S3, \\text{actions})) = 0 + 0.5 \\times 100 = 50$.
    * 이제 Q-테이블의 $Q(S2, a23)$ 값은 50이 됩니다.

3.  **세 번째 에피소드**: 에이전트가 S4에서 시작하여 S1을 거쳐 S2로 이동했다고 가정합니다.

    * S1에서 S2로 이동할 때(행동 a12), 에이전트는 $Q(S1, a12)$ 값을 업데이트합니다.
    * 도착한 상태 S2에서 얻을 수 있는 최대 Q값을 참조합니다. 이전 단계에서 S2의 최대 Q값은 50($Q(S2, a23)$)임을 학습했습니다.
    * 업데이트 계산: $Q(S1, a12) = r + \\gamma \\max(Q(S2, \\text{actions})) = 0 + 0.5 times 50 = 25$.
    * 테이블은 $Q(S1, a12) = 25$로 업데이트됩니다.

**수많은 반복 후**:
이 과정을 여러 번 반복하면 목표 지점의 보상 100이 전체 Q-테이블에 점차 역으로 전파됩니다. 최종적으로 수렴된 테이블은 다음과 같을 수 있습니다.

| **s,a** | **Q(s,a)** | **해석** |
| :--- | :--- | :--- |
| S1,a12 | 25 | S1->S2 이동의 가치는 25입니다. |
| S1,a14 | 25 | S1->S4 이동의 가치는 25입니다. |
| S2,a23 | 50 | S2->S3 이동의 가치는 50입니다. |
| S2,a25 | 50 | S2->S5 이동의 가치는 50입니다. |
| S3,a36 | 100 | S3에서 목표 지점으로 바로 이동합니다. |
| S4,a45 | 50 | S4->S5 이동의 가치는 50입니다. |
| S5,a56 | 100 | S5에서 목표 지점으로 바로 이동합니다. |
| ... | ... | (다른 값들도 업데이트됨) |

이 테이블이 학습되면, 에이전트는 어떤 상태에서든 가장 높은 Q값을 가진 행동을 선택하여 최적의 경로를 찾을 수 있습니다.


## 5. Q-러닝의 적용

Q-러닝과 그 변형 알고리즘들은 간단한 게임, 로봇의 경로 탐색, 자원 관리 등 다양한 분야에서 사용됩니다. 발표 자료에서는 에이전트가 시작 지점(빨간 사각형)에서 목표 지점(초록 사각형)으로 이동하는 방법을 학습하는 간단한 그리드 월드 문제가 예시로 제시되었습니다.